
# Reflective Activity 1: Ethics in Computing

## Title: Governing Generative AI – Towards Ethical Consensus and Practical Oversight

The resurgence of artificial intelligence—particularly generative AI—has marked a pivotal shift in global technological development. As Correa et al. (2023) emphasise, the challenge today is not the absence of governance frameworks, but rather the divergence of values and approaches among stakeholders. This reflective piece evaluates global efforts in AI governance, assesses ethical, social, and legal implications, and proposes a future-oriented course of action grounded in my professional context as a computing practitioner.

Generative AI, such as large language models and diffusion-based image generators, has already disrupted sectors from creative design to software development. However, the proliferation of new technologies is outpacing the development of legal frameworks. National AI standards are fragmented, and normative terminology is abstract, making it impossible to develop a unified ethical framework (Correa et al., 2023). For example, the United States and China approach AI from an innovation-first perspective, while the EU prioritises individual rights and safety through frameworks such as the AI Act. This divergence reinforces a global patchwork of policy responses, where consensus is elusive, and enforcement is inconsistent.

Deckard (2023) presents new concerns by underlining how firms frequently behave as "quasi-regulators," setting de facto standards through their influence. For example, the release of ChatGPT by OpenAI triggered a spike in integration before there was any meaningful debate about its implications. Although the technology was outstanding, it also presented significant hazards—from the amplification of misinformation to the illegal use of data—demonstrating that market-driven innovation is not enough to protect the public.

As a student of data science and a computing professional, I find these developments intriguing but dubious.

My own work intersects with performance dashboards and civic data, and I increasingly recognise the obligation to embed ethical awareness at every stage of system design. If I were to introduce generative AI into the analytics or public reporting processes at Marlborough Highways, I would face ethical trade-offs—such as the risk of inadvertently introducing bias or misleading representations through summarised outputs. Therefore, one of the most important aspects of my professional development is learning how to strike a balance between accountability and creativity.

Transparency and data minimisation requirements are now legally imposed by the General Data Protection Regulation (GDPR) of the UK and the EU, but implementing them in the context of black-box models is still challenging. As Deckard (2023) shows, some organisations avoid criticism by referring to AI-generated content as "augmented output," shifting responsibility for erroneous information to customers. In my opinion, this undermines accountability and emphasises the significance of auditable AI pipelines, particularly in use cases involving the public sector.

The gap in AI literacy throughout society is growing. According to Correa et al. (2023), the lack of a common language and public involvement exacerbates inequality, since people who understand AI can use it, while others are vulnerable to manipulation, job loss, or exclusion from the digital world. I'm concerned that computer scientists may unwittingly increase this divide by developing systems that favour efficiency over usability. For example, how can we ensure that the visual narratives or auto-summaries generated by performance dashboards are inclusive and impartial? The work on infrastructure transparency is becoming increasingly relevant to this topic.

In my perspective, the ACM Code of Ethics (ACM, 2018) provides a good starting point for professionals. While vital, principles such as damage prevention (1.2), open and honest communication (1.3), and privacy protection (1.6) are insufficient unless they are reinforced through training and workplace culture. The British Computer Society's Code of Conduct (2022) underlines the need to "promote equal access to the benefits of IT" while also defending the public interest. In reality, this could mean assisting with algorithmic impact evaluations or human-in-the-loop processes, both of which I believe will be required for future digital transformation.

Given these considerations, my proposed solution supports Correa et al.'s call for additional standards and interoperability in AI governance. I believe the UK should take the lead in developing a hybrid regulatory system that combines the flexibility of industry-led sandboxing with the enforcement of the EU's rights-based approach. This compromise strategy may promote creativity while respecting moral ideals. To demystify AI processes and enable user accountability, I believe it would be appropriate to include publicly accessible descriptions of algorithmic logic and model intent within key platforms.

Additionally, I advocate for mandatory transparency reports for any deployed generative AI in the public sector. These reports should provide information about the training data sources, known biases, use contexts, and redress mechanisms. Users would be empowered, developers would be held to professional standards, and legal compliance would be increased. As a prospective data specialist, I would push for incorporating these practices within our procurement processes and procedures, as they should be embedded from the start of the software development lifecycle. Fairness and openness must become fundamental design principles rather than after-the-fact features.

Finally, the reflective learning component of this work has altered my perspective on my role in the computing industry. I now regard ethics as a constant commitment to critically analyse the systems we establish, rather than as an afterthought or a compliance obstacle. I plan to carry this mindset forward into my MSc project, especially if it involves data visualisation or AI-driven summarisation. The ramifications of algorithmic opacity, user autonomy, and public accountability will all be carefully considered as I think about implementing new technologies in my future work.

In conclusion, generative AI has an impact on the core ideas of computing practice, as well as professional standards and legal frameworks. To navigate this landscape, we must build consensus, exercise technical rigour, and engage in ethical reflexivity. By combining robust frameworks like the GDPR and the BCS Code with open design and international collaboration, we may move closer to a world in which generative AI enhances—rather than erodes—trust, equity, and public value.

## References

- ACM (2018). Code of Ethics and Professional Conduct. https://www.acm.org/code-of-ethics  
- BCS (2022). Code of Conduct for BCS Members. https://www.bcs.org/media/2211/bcs-code-of-conduct.pdf  
- Correa, C., Ali, S., & Grimmelmann, J. (2023). Mapping AI Governance Landscapes: A Comparative Study of National and Multistakeholder Frameworks. *AI & Society.*  
- Deckard, M. (2023). Corporate Capture in AI Governance: When Technology Outpaces Regulation. *Journal of Technology Ethics*, 17(3), pp. 45–61.
